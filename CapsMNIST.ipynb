{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(input_tensor):\n",
    "    squared_norm = tf.reduce_sum((input_tensor ** 2), axis=-1, keepdims=True)\n",
    "    output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * tf.sqrt(squared_norm))\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(net_input, no_primary_capsules=8, no_digit_capsules=10, routes = 32*6*6):\n",
    "    # (batch, channel, height, width) or (batch, height, width, channel)\n",
    "    \n",
    "    with tf.variable_scope('ReLU_Conv1'):\n",
    "        conv_out = tf.layers.conv2d(inputs=net_input, filters=256, kernel_size=9, activation=tf.nn.relu)\n",
    "    print(\"ReLU Conv1 OUT: {}\".format(tf.shape(conv_out)))\n",
    "    print(\"ReLU Conv1 OUT: {}\".format(conv_out.get_shape()))\n",
    "    # PRIMARY CAPSULE\n",
    "    primary_capsules = []\n",
    "    for i in range(no_primary_capsules):\n",
    "        with tf.variable_scope('primary_capsule_{}'.format(i)):\n",
    "            primary_capsules.append(tf.transpose(tf.layers.conv2d(inputs=conv_out, filters=32, kernel_size=9, strides=(2,2), padding='valid'), [0,3,1,2]))\n",
    "    primary_capsules = tf.stack(primary_capsules, axis=1)\n",
    "    print(\"PRIMARY CAPSULES (before reshape): {}\".format(tf.shape(primary_capsules)))\n",
    "    print(\"PRIMARY CAPSULES (before reshape): {}\".format(primary_capsules.get_shape()))\n",
    "    primary_capsules = tf.reshape(primary_capsules, [tf.shape(conv_out)[0],32*6*6,-1])\n",
    "    print(\"PRIMARY CAPSULES (before squash): {}\".format(tf.shape(primary_capsules)))\n",
    "    print(\"PRIMARY CAPSULES (before squash): {}\".format(primary_capsules.get_shape()))\n",
    "    primary_capsules = squash(primary_capsules)\n",
    "    print(\"PRIMARY CAPSULES OUT: {}\".format(tf.shape(primary_capsules)))\n",
    "    print(\"PRIMARY CAPSULES OUT: {}\".format(primary_capsules.get_shape()))\n",
    "\n",
    "    # DIGIT CAPSULE\n",
    "    batch_size = tf.shape(primary_capsules)[0]\n",
    "    x = tf.expand_dims(tf.stack([primary_capsules] * no_digit_capsules, axis=2), axis=4)\n",
    "    print('X SHAPE: {}'.format(tf.shape(x)))\n",
    "    with tf.variable_scope(\"transformation_matrix_weights\", reuse=tf.AUTO_REUSE):\n",
    "        W = tf.get_variable('W', [1, routes, no_digit_capsules, 16, 8], trainable=True)\n",
    "    with tf.variable_scope(\"similarity_score\", reuse=tf.AUTO_REUSE):\n",
    "        b_ij = tf.get_variable('b', [1, routes, no_digit_capsules, 1], initializer=tf.zeros_initializer)\n",
    "    \n",
    "    W_batch = tf.tile(W, [batch_size,1,1,1,1])\n",
    "    print(\"X SHAPE: {}\".format(tf.shape(x)))\n",
    "    print(\"X SHAPE: {}\".format(x.get_shape()))\n",
    "    print(\"W BATCH SHAPE: {}\".format(tf.shape(W_batch)))\n",
    "    print(\"W BATCH SHAPE: {}\".format(W_batch.get_shape()))\n",
    "    \n",
    "    # u_hat = \\hat{u}_{j|i} -- prediction vector\n",
    "    u_hat = tf.matmul(W_batch,x)\n",
    "    print('u_hat SHAPE: {}'.format(tf.shape(u_hat)))\n",
    "    print('u_hat SHAPE: {}'.format(u_hat.get_shape()))\n",
    "    \n",
    "    num_iterations = 3\n",
    "    for iteration in range(num_iterations):\n",
    "        # c_ij -- coupling coefficients\n",
    "        c_ij = tf.nn.softmax(b_ij)\n",
    "        c_ij = tf.expand_dims(tf.concat(tf.tile(c_ij, [batch_size, 1, 1, 1]), axis=0), axis=4)\n",
    "        \n",
    "        s_j = tf.reduce_sum((c_ij * u_hat), axis=1, keepdims=True)\n",
    "        # v_j -- output | activity vector\n",
    "        v_j = squash(s_j)\n",
    "        \n",
    "        if iteration < num_iterations - 1:\n",
    "            a_ij = tf.matmul(tf.transpose(u_hat, [0,1,2,4,3]), tf.concat([v_j] * routes, axis=1))\n",
    "            b_ij = b_ij + tf.reduce_mean(tf.squeeze(a_ij, axis=4), axis=0, keepdims=True)\n",
    "    digit_output = tf.squeeze(v_j, axis=1)\n",
    "    print('DIGIT CAPSULE OUT: {}'.format(tf.shape(digit_output)))\n",
    "    print('DIGIT CAPSULE OUT: {}'.format(digit_output.get_shape()))\n",
    "            \n",
    "    # DECODER\n",
    "    classes = tf.reduce_sum(tf.sqrt(digit_output ** 2), axis=2)\n",
    "    classes = tf.nn.softmax(classes)\n",
    "    \n",
    "    max_length_indices = tf.argmax(classes, axis=1)\n",
    "    masked = tf.eye(10)\n",
    "    masked = tf.gather(masked, axis=0, indices=tf.squeeze(max_length_indices, axis=1))\n",
    "    decoder_input = tf.reshape(digit_output * masked[:, :, None, None], [tf.shape(digit_output)[0], 10*16*1])\n",
    "    print('DECODER INPUT: {}'.format(tf.shape(decoder_input)))\n",
    "    print('DECODER INPUT: {}'.format(decoder_input.get_shape()))\n",
    "      \n",
    "    net = tf.layers.dense(inputs=decoder_input, units=512, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(inputs=decoder_input, units=1024, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(inputs=decoder_input, units=784, activation=tf.nn.sigmoid)\n",
    "    \n",
    "    decoder_output = tf.reshape(net, [-1, 1, 28, 28])\n",
    "    \n",
    "    return digit_output, decoder_output, masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(data, x, target, reconstructions):\n",
    "    loss_val = margin_loss(x, target)\n",
    "    recon_loss_val = reconstruction_loss(data, reconstructions)\n",
    "    return loss_val + recon_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def margin_loss(x, labels, size_average=True):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    v_c = tf.sqrt(tf.reduce_sum((x ** 2), axis=2, keepdims=True))\n",
    "    \n",
    "    left = tf.reshape(tf.nn.relu(0.9 - v_c), [batch_size, -1])\n",
    "    right = tf.reshape(tf.nn.relu(v_c - 0.1), [batch_size, -1])\n",
    "    \n",
    "    loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruction_loss(data, reconstructions):\n",
    "    labels = tf.reshape(reconstructions, [tf.shape(reconstructions)[0], -1])\n",
    "    target = tf.reshape(data, [tf.shape(reconstructions)[0], -1])\n",
    "    loss = tf.losses.mean_squared_error(labels, target)\n",
    "    return loss * 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(loss, learning_rate=0.001):\n",
    "    return tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU Conv1 OUT: Tensor(\"Shape:0\", shape=(4,), dtype=int32)\n",
      "ReLU Conv1 OUT: (?, 20, 20, 256)\n",
      "PRIMARY CAPSULES (before reshape): Tensor(\"Shape_1:0\", shape=(5,), dtype=int32)\n",
      "PRIMARY CAPSULES (before reshape): (?, 8, 32, 6, 6)\n",
      "PRIMARY CAPSULES (before squash): Tensor(\"Shape_3:0\", shape=(3,), dtype=int32)\n",
      "PRIMARY CAPSULES (before squash): (?, 1152, ?)\n",
      "PRIMARY CAPSULES OUT: Tensor(\"Shape_4:0\", shape=(3,), dtype=int32)\n",
      "PRIMARY CAPSULES OUT: (?, 1152, ?)\n",
      "X SHAPE: Tensor(\"Shape_6:0\", shape=(5,), dtype=int32)\n",
      "X SHAPE: Tensor(\"Shape_7:0\", shape=(5,), dtype=int32)\n",
      "X SHAPE: (?, 1152, 10, ?, 1)\n",
      "W BATCH SHAPE: Tensor(\"Shape_8:0\", shape=(5,), dtype=int32)\n",
      "W BATCH SHAPE: (?, 1152, 10, 16, 8)\n",
      "u_hat SHAPE: Tensor(\"Shape_9:0\", shape=(5,), dtype=int32)\n",
      "u_hat SHAPE: (?, 1152, 10, 16, 1)\n",
      "DIGIT CAPSULE OUT: Tensor(\"Shape_16:0\", shape=(4,), dtype=int32)\n",
      "DIGIT CAPSULE OUT: (?, 10, 16, 1)\n",
      "DECODER INPUT: Tensor(\"Shape_20:0\", shape=(2,), dtype=int32)\n",
      "DECODER INPUT: (?, 160)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "input_pl = tf.placeholder(shape=[None,28,28,1], dtype=tf.float32)\n",
    "target_pl = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "net = forward(input_pl)\n",
    "loss_op = loss(input_pl, net[0], target_pl, net[1])\n",
    "train_op = train(loss_op)\n",
    "\n",
    "print()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(1000):\n",
    "        numpy_input = np.random.rand(64,28,28,1)\n",
    "        numpy_target = np.random.randint(low=0, high=2, size=(64,1))\n",
    "        loss_val, res = sess.run([loss_op, train_op], feed_dict={input_pl: numpy_input, target_pl: numpy_target})\n",
    "        print('Loss: {}'.format(loss_val))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
